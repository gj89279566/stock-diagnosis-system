#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ·±åº¦æ–°é—»è°ƒè¯•è„šæœ¬
æ£€æŸ¥æ–°æµªè´¢ç»æ–°é—»é¡µé¢çš„å…·ä½“å†…å®¹ï¼Œæ‰¾å‡ºæ–°é—»è·å–å¤±è´¥çš„åŸå› 
"""

import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime

def deep_debug_sina_news():
    """æ·±åº¦è°ƒè¯•æ–°æµªè´¢ç»æ–°é—»é¡µé¢"""
    print("=== æ·±åº¦è°ƒè¯•æ–°æµªè´¢ç»æ–°é—»é¡µé¢ ===")
    
    stock_code = "sh603259"
    url = "https://vip.stock.finance.sina.com.cn/corp/view/vCB_AllNewsStock.php"
    params = {"symbol": stock_code, "Page": 1}
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
    
    try:
        resp = requests.get(url, params=params, headers=headers, timeout=10)
        if resp.status_code == 200:
            resp.encoding = 'gbk'
            content = resp.text
            
            print(f"é¡µé¢æ ‡é¢˜: {re.search(r'<title>(.*?)</title>', content, re.IGNORECASE).group(1) if re.search(r'<title>(.*?)</title>', content, re.IGNORECASE) else 'æœªæ‰¾åˆ°æ ‡é¢˜'}")
            
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½åŒ…å«æ–°é—»çš„å…ƒç´ 
            soup = BeautifulSoup(content, 'html.parser')
            
            # 1. æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
            links = soup.find_all('a', href=True)
            news_links = []
            for link in links:
                href = link.get('href', '')
                text = link.get_text().strip()
                if any(keyword in href.lower() for keyword in ['news', 'notice', 'announcement', 'report']):
                    news_links.append((text, href))
            
            print(f"\næ‰¾åˆ° {len(news_links)} ä¸ªå¯èƒ½çš„æ–°é—»é“¾æ¥:")
            for text, href in news_links[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
                print(f"  {text} -> {href}")
            
            # 2. æŸ¥æ‰¾æ‰€æœ‰è¡¨æ ¼
            tables = soup.find_all('table')
            print(f"\næ‰¾åˆ° {len(tables)} ä¸ªè¡¨æ ¼:")
            
            for i, table in enumerate(tables):
                print(f"\nè¡¨æ ¼ {i+1}:")
                rows = table.find_all('tr')
                print(f"  è¡Œæ•°: {len(rows)}")
                
                if len(rows) > 0:
                    # æ˜¾ç¤ºå‰å‡ è¡Œçš„å†…å®¹
                    for j, row in enumerate(rows[:3]):
                        cells = row.find_all(['td', 'th'])
                        cell_texts = [cell.get_text().strip() for cell in cells]
                        print(f"  è¡Œ {j+1}: {cell_texts}")
            
            # 3. æŸ¥æ‰¾ç‰¹å®šçš„æ–°é—»å®¹å™¨
            news_containers = soup.find_all(['div', 'ul', 'ol'], class_=re.compile(r'news|list|item'))
            print(f"\næ‰¾åˆ° {len(news_containers)} ä¸ªå¯èƒ½çš„æ–°é—»å®¹å™¨:")
            
            for i, container in enumerate(news_containers[:5]):
                print(f"\nå®¹å™¨ {i+1} (class={container.get('class', 'æ— ')}):")
                print(f"  å†…å®¹: {container.get_text()[:200]}...")
            
            # 4. æŸ¥æ‰¾æ—¥æœŸæ¨¡å¼
            date_pattern = re.compile(r'\d{4}-\d{2}-\d{2}')
            dates = date_pattern.findall(content)
            print(f"\næ‰¾åˆ° {len(dates)} ä¸ªæ—¥æœŸ:")
            for date in dates[:10]:
                print(f"  {date}")
            
            # 5. æ£€æŸ¥é¡µé¢æ˜¯å¦åŒ…å«"æ²¡æœ‰"ã€"æš‚æ— "ç­‰å…³é”®è¯
            no_data_keywords = ['æ²¡æœ‰', 'æš‚æ— ', 'æ— æ•°æ®', 'empty', 'no data']
            for keyword in no_data_keywords:
                if keyword in content:
                    print(f"\nâš ï¸ é¡µé¢åŒ…å«å…³é”®è¯: {keyword}")
            
            # 6. æŸ¥æ‰¾æ–°é—»åˆ—è¡¨çš„ç‰¹å®šæ¨¡å¼
            # æ–°æµªè´¢ç»æ–°é—»é€šå¸¸åœ¨è¿™ä¸ªä½ç½®
            news_section = soup.find('div', {'id': 'con02-0'})
            if news_section:
                print(f"\næ‰¾åˆ°æ–°é—»åŒºåŸŸ (con02-0):")
                print(f"å†…å®¹: {news_section.get_text()[:500]}...")
            else:
                print(f"\nâŒ æœªæ‰¾åˆ°æ–°é—»åŒºåŸŸ (con02-0)")
            
            # 7. å°è¯•å…¶ä»–å¯èƒ½çš„æ–°é—»åŒºåŸŸ
            possible_news_areas = [
                soup.find('div', {'id': 'con02-1'}),
                soup.find('div', {'id': 'con02-2'}),
                soup.find('div', {'class': 'news_list'}),
                soup.find('ul', {'class': 'news_list'}),
            ]
            
            for i, area in enumerate(possible_news_areas):
                if area:
                    print(f"\næ‰¾åˆ°å¯èƒ½çš„æ–°é—»åŒºåŸŸ {i+1}:")
                    print(f"å†…å®¹: {area.get_text()[:300]}...")
            
            # 8. æ£€æŸ¥æ˜¯å¦æœ‰åˆ†é¡µä¿¡æ¯
            pagination = soup.find_all('a', href=re.compile(r'Page='))
            if pagination:
                print(f"\næ‰¾åˆ°åˆ†é¡µé“¾æ¥: {len(pagination)} ä¸ª")
                for page_link in pagination[:5]:
                    print(f"  {page_link.get_text()} -> {page_link.get('href')}")
            
            # 9. ä¿å­˜é¡µé¢å†…å®¹åˆ°æ–‡ä»¶ä»¥ä¾¿è¿›ä¸€æ­¥åˆ†æ
            with open('sina_news_page.html', 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"\nğŸ“„ é¡µé¢å†…å®¹å·²ä¿å­˜åˆ° sina_news_page.html")
            
        else:
            print(f"âŒ HTTPè¯·æ±‚å¤±è´¥: {resp.status_code}")
            
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")

def test_different_stock_codes():
    """æµ‹è¯•ä¸åŒè‚¡ç¥¨ä»£ç çš„æ–°é—»è·å–"""
    print("\n=== æµ‹è¯•ä¸åŒè‚¡ç¥¨ä»£ç çš„æ–°é—»è·å– ===")
    
    test_stocks = [
        {"code": "sh603259", "name": "è¯æ˜åº·å¾·"},
        {"code": "sh600519", "name": "è´µå·èŒ…å°"},  # çƒ­é—¨è‚¡ç¥¨
        {"code": "sz000001", "name": "å¹³å®‰é“¶è¡Œ"},  # æ·±å¸‚è‚¡ç¥¨
        {"code": "sh000001", "name": "ä¸Šè¯æŒ‡æ•°"},  # æŒ‡æ•°
    ]
    
    for stock in test_stocks:
        print(f"\næµ‹è¯• {stock['name']} ({stock['code']}):")
        
        url = "https://vip.stock.finance.sina.com.cn/corp/view/vCB_AllNewsStock.php"
        params = {"symbol": stock['code'], "Page": 1}
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
        
        try:
            resp = requests.get(url, params=params, headers=headers, timeout=10)
            if resp.status_code == 200:
                resp.encoding = 'gbk'
                content = resp.text
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«æ–°é—»è¡¨æ ¼
                soup = BeautifulSoup(content, 'html.parser')
                tables = soup.find_all('table')
                
                news_count = 0
                for table in tables:
                    rows = table.find_all('tr')
                    for row in rows[1:]:  # è·³è¿‡è¡¨å¤´
                        cells = row.find_all('td')
                        if len(cells) >= 2:
                            date_text = cells[0].get_text().strip()
                            if re.match(r'\d{4}-\d{2}-\d{2}', date_text):
                                news_count += 1
                
                print(f"  æ‰¾åˆ° {news_count} æ¡æ–°é—»")
                
                if news_count == 0:
                    # æ£€æŸ¥é¡µé¢æ˜¯å¦åŒ…å«"æ²¡æœ‰"ç­‰å…³é”®è¯
                    if 'æ²¡æœ‰' in content or 'æš‚æ— ' in content:
                        print(f"  âœ… é¡µé¢æ˜¾ç¤ºæ²¡æœ‰ç›¸å…³æ–°é—»")
                    else:
                        print(f"  â“ é¡µé¢å†…å®¹å¼‚å¸¸")
                else:
                    print(f"  âœ… æˆåŠŸè·å–æ–°é—»")
                    
            else:
                print(f"  âŒ HTTPè¯·æ±‚å¤±è´¥: {resp.status_code}")
                
        except Exception as e:
            print(f"  âŒ è¯·æ±‚å¼‚å¸¸: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å¼€å§‹æ·±åº¦è°ƒè¯•æ–°é—»è·å–...")
    print(f"â° è°ƒè¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # æ·±åº¦è°ƒè¯•æ–°æµªè´¢ç»æ–°é—»é¡µé¢
    deep_debug_sina_news()
    
    # æµ‹è¯•ä¸åŒè‚¡ç¥¨ä»£ç 
    test_different_stock_codes()
    
    print("\nâœ… æ·±åº¦è°ƒè¯•å®Œæˆï¼")
    print("\nğŸ’¡ å»ºè®®:")
    print("1. æ£€æŸ¥ sina_news_page.html æ–‡ä»¶äº†è§£é¡µé¢ç»“æ„")
    print("2. æ ¹æ®é¡µé¢ç»“æ„è°ƒæ•´æ–°é—»è§£æé€»è¾‘")
    print("3. è€ƒè™‘ä½¿ç”¨å…¶ä»–æ–°é—»æºä½œä¸ºè¡¥å……")

if __name__ == "__main__":
    main() 