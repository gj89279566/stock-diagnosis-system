#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šæ•°æ®æºæµ‹è¯•è„šæœ¬
æµ‹è¯•æ‰€æœ‰æ–°é—»æ•°æ®æºçš„æ•ˆæœï¼Œå¹¶ç”Ÿæˆç»¼åˆæŠ¥å‘Š
"""

import requests
from bs4 import BeautifulSoup
import json
import re
from datetime import datetime
import time

def test_sina_news(stock_code="sh603259"):
    """æµ‹è¯•æ–°æµªè´¢ç»æ–°é—»"""
    print("=== æµ‹è¯•æ–°æµªè´¢ç»æ–°é—» ===")
    
    url = "https://vip.stock.finance.sina.com.cn/corp/view/vCB_AllNewsStock.php"
    params = {"symbol": stock_code, "Page": 1}
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
    
    try:
        resp = requests.get(url, params=params, headers=headers, timeout=10)
        if resp.status_code == 200:
            resp.encoding = 'gbk'
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            news_container = soup.find('div', class_='datelist')
            if news_container:
                news_pattern = r'(\d{4}-\d{2}-\d{2})\s+(\d{2}:\d{2})\s+<a[^>]*>([^<]+)</a>'
                news_matches = re.findall(news_pattern, str(news_container))
                
                news_list = []
                for match in news_matches:
                    date_str = match[0]
                    time_str = match[1]
                    title = match[2].strip()
                    full_date = f"{date_str} {time_str}"
                    news_list.append((full_date, title))
                
                print(f"âœ… æ–°æµªè´¢ç»: æˆåŠŸè·å– {len(news_list)} æ¡æ–°é—»")
                return news_list
            else:
                print("âŒ æ–°æµªè´¢ç»: æœªæ‰¾åˆ°æ–°é—»å®¹å™¨")
                return []
        else:
            print(f"âŒ æ–°æµªè´¢ç»: HTTPé”™è¯¯ {resp.status_code}")
            return []
    except Exception as e:
        print(f"âŒ æ–°æµªè´¢ç»: {e}")
        return []

def test_eastmoney_news(stock_code="sh603259"):
    """æµ‹è¯•ä¸œæ–¹è´¢å¯Œæ–°é—»"""
    print("\n=== æµ‹è¯•ä¸œæ–¹è´¢å¯Œæ–°é—» ===")
    
    url = "http://np-anotice-stock.eastmoney.com/api/security/announcement/getAnnouncementList"
    params = {"cb": "jQuery", "pageSize": 20, "pageIndex": 1, "stock": stock_code}
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
    
    try:
        resp = requests.get(url, params=params, headers=headers, timeout=10)
        if resp.status_code == 200 and resp.text:
            content = resp.text
            news_list = []
            
            # å°è¯•è§£æJSONPæ ¼å¼
            if content.startswith('jQuery(') and content.endswith(')'):
                json_str = content[7:-1]
                try:
                    data = json.loads(json_str)
                    if 'data' in data and 'list' in data['data']:
                        news_items = data['data']['list']
                        for item in news_items:
                            date_str = item.get('notice_date', '')
                            title = item.get('title', '')
                            if date_str and title:
                                news_list.append((date_str, title))
                        
                        print(f"âœ… ä¸œæ–¹è´¢å¯Œ: æˆåŠŸè·å– {len(news_list)} æ¡å…¬å‘Š")
                        return news_list
                except json.JSONDecodeError:
                    pass
            
            # å°è¯•ç›´æ¥è§£æJSON
            try:
                data = resp.json()
                if 'data' in data and 'list' in data['data']:
                    news_items = data['data']['list']
                    for item in news_items:
                        date_str = item.get('notice_date', '')
                        title = item.get('title', '')
                        if date_str and title:
                            news_list.append((date_str, title))
                    
                    print(f"âœ… ä¸œæ–¹è´¢å¯Œ: æˆåŠŸè·å– {len(news_list)} æ¡å…¬å‘Š")
                    return news_list
            except json.JSONDecodeError:
                pass
            
            print("âŒ ä¸œæ–¹è´¢å¯Œ: è§£æå¤±è´¥")
            return []
        else:
            print(f"âŒ ä¸œæ–¹è´¢å¯Œ: å“åº”ä¸ºç©ºæˆ–HTTPé”™è¯¯ {resp.status_code}")
            return []
    except Exception as e:
        print(f"âŒ ä¸œæ–¹è´¢å¯Œ: {e}")
        return []

def test_xueqiu_news(stock_code="sh603259"):
    """æµ‹è¯•é›ªçƒæ–°é—»"""
    print("\n=== æµ‹è¯•é›ªçƒæ–°é—» ===")
    
    url = f"https://xueqiu.com/statuses/search.json?count=20&comment=0&source=all&sort=time&page=1&stock={stock_code}"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        "Referer": "https://xueqiu.com/",
        "Accept": "application/json, text/plain, */*",
        "X-Requested-With": "XMLHttpRequest"
    }
    
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        if resp.status_code == 200:
            try:
                data = resp.json()
                if 'list' in data:
                    news_items = data['list']
                    news_list = []
                    
                    for item in news_items:
                        created_at = item.get('created_at', '')
                        title = item.get('title', '')
                        if created_at and title:
                            try:
                                from datetime import datetime
                                dt = datetime.fromtimestamp(created_at / 1000)
                                date_str = dt.strftime('%Y-%m-%d %H:%M')
                                news_list.append((date_str, title))
                            except:
                                news_list.append((str(created_at), title))
                    
                    print(f"âœ… é›ªçƒ: æˆåŠŸè·å– {len(news_list)} æ¡æ–°é—»")
                    return news_list
                else:
                    print("âŒ é›ªçƒ: æ•°æ®ç»“æ„ä¸ç¬¦åˆé¢„æœŸ")
                    return []
            except json.JSONDecodeError as e:
                print(f"âŒ é›ªçƒ: JSONè§£æå¤±è´¥ {e}")
                return []
        else:
            print(f"âŒ é›ªçƒ: HTTPé”™è¯¯ {resp.status_code}")
            return []
    except Exception as e:
        print(f"âŒ é›ªçƒ: {e}")
        return []

def test_ths_news(stock_code="sh603259"):
    """æµ‹è¯•åŒèŠ±é¡ºæ–°é—»"""
    print("\n=== æµ‹è¯•åŒèŠ±é¡ºæ–°é—» ===")
    
    url = f"http://news.10jqka.com.cn/tapp/news/push/stock/{stock_code}/"
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
    
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        if resp.status_code == 200:
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # æŸ¥æ‰¾æ–°é—»åˆ—è¡¨
            news_items = soup.find_all(['div', 'li'], class_=re.compile(r'news|item|list'))
            news_list = []
            
            for item in news_items:
                date_elem = item.find(['span', 'div'], class_=re.compile(r'date|time'))
                title_elem = item.find('a')
                
                if date_elem and title_elem:
                    date_str = date_elem.get_text().strip()
                    title = title_elem.get_text().strip()
                    if date_str and title:
                        news_list.append((date_str, title))
            
            if news_list:
                print(f"âœ… åŒèŠ±é¡º: æˆåŠŸè·å– {len(news_list)} æ¡æ–°é—»")
                return news_list
            else:
                print("âŒ åŒèŠ±é¡º: æœªæ‰¾åˆ°æ–°é—»")
                return []
        else:
            print(f"âŒ åŒèŠ±é¡º: HTTPé”™è¯¯ {resp.status_code}")
            return []
    except Exception as e:
        print(f"âŒ åŒèŠ±é¡º: {e}")
        return []

def test_jrj_news(stock_code="sh603259"):
    """æµ‹è¯•é‡‘èç•Œæ–°é—»"""
    print("\n=== æµ‹è¯•é‡‘èç•Œæ–°é—» ===")
    
    url = f"http://stock.jrj.com.cn/report/{stock_code}/"
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
    
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        if resp.status_code == 200:
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # æŸ¥æ‰¾æ–°é—»åˆ—è¡¨
            news_items = soup.find_all(['div', 'li'], class_=re.compile(r'news|item|list'))
            news_list = []
            
            for item in news_items:
                date_elem = item.find(['span', 'div'], class_=re.compile(r'date|time'))
                title_elem = item.find('a')
                
                if date_elem and title_elem:
                    date_str = date_elem.get_text().strip()
                    title = title_elem.get_text().strip()
                    if date_str and title:
                        news_list.append((date_str, title))
            
            if news_list:
                print(f"âœ… é‡‘èç•Œ: æˆåŠŸè·å– {len(news_list)} æ¡æ–°é—»")
                return news_list
            else:
                print("âŒ é‡‘èç•Œ: æœªæ‰¾åˆ°æ–°é—»")
                return []
        else:
            print(f"âŒ é‡‘èç•Œ: HTTPé”™è¯¯ {resp.status_code}")
            return []
    except Exception as e:
        print(f"âŒ é‡‘èç•Œ: {e}")
        return []

def generate_comprehensive_report(all_news, stock_code="sh603259"):
    """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
    print("\n" + "="*60)
    print("ğŸ“Š å¤šæ•°æ®æºç»¼åˆæŠ¥å‘Š")
    print("="*60)
    
    # ç»Ÿè®¡å„æ•°æ®æºè´¡çŒ®
    source_stats = {}
    total_news = 0
    
    for source, news_list in all_news.items():
        if news_list:
            source_stats[source] = len(news_list)
            total_news += len(news_list)
        else:
            source_stats[source] = 0
    
    print(f"\nğŸ“ˆ æ•°æ®æºç»Ÿè®¡:")
    for source, count in source_stats.items():
        status = "âœ…" if count > 0 else "âŒ"
        print(f"  {status} {source}: {count} æ¡")
    
    print(f"\nğŸ“Š æ€»è®¡: {total_news} æ¡æ–°é—»")
    
    # å»é‡ç»Ÿè®¡
    all_titles = set()
    unique_news = []
    
    for source, news_list in all_news.items():
        for date, title in news_list:
            if title not in all_titles:
                unique_news.append((date, title, source))
                all_titles.add(title)
    
    print(f"ğŸ¯ å»é‡å: {len(unique_news)} æ¡æ–°é—»")
    
    # æŒ‰æ—¶é—´æ’åº
    unique_news.sort(key=lambda x: x[0], reverse=True)
    
    print(f"\nğŸ“° æœ€æ–°æ–°é—» (å‰10æ¡):")
    for i, (date, title, source) in enumerate(unique_news[:10]):
        print(f"  {i+1}. [{source}] {date}: {title[:60]}...")
    
    # æ•°æ®æºå¯é æ€§è¯„ä¼°
    print(f"\nğŸ” æ•°æ®æºå¯é æ€§è¯„ä¼°:")
    reliable_sources = []
    for source, count in source_stats.items():
        if count > 0:
            reliable_sources.append(source)
            print(f"  âœ… {source}: å¯ç”¨ ({count} æ¡)")
        else:
            print(f"  âŒ {source}: ä¸å¯ç”¨")
    
    print(f"\nğŸ’¡ å»ºè®®:")
    if reliable_sources:
        print(f"  æ¨èä½¿ç”¨: {', '.join(reliable_sources)}")
        if len(reliable_sources) > 1:
            print(f"  å»ºè®®ç»„åˆä½¿ç”¨å¤šä¸ªæ•°æ®æºä»¥è·å¾—æ›´å…¨é¢çš„ä¿¡æ¯")
    else:
        print(f"  æ‰€æœ‰æ•°æ®æºéƒ½ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
    
    return unique_news, source_stats

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å¼€å§‹å¤šæ•°æ®æºæµ‹è¯•...")
    print(f"â° æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    stock_code = "sh603259"  # è¯æ˜åº·å¾·
    
    # æµ‹è¯•æ‰€æœ‰æ•°æ®æº
    all_news = {}
    
    all_news['æ–°æµªè´¢ç»'] = test_sina_news(stock_code)
    time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
    
    all_news['ä¸œæ–¹è´¢å¯Œ'] = test_eastmoney_news(stock_code)
    time.sleep(1)
    
    all_news['é›ªçƒ'] = test_xueqiu_news(stock_code)
    time.sleep(1)
    
    all_news['åŒèŠ±é¡º'] = test_ths_news(stock_code)
    time.sleep(1)
    
    all_news['é‡‘èç•Œ'] = test_jrj_news(stock_code)
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    unique_news, source_stats = generate_comprehensive_report(all_news, stock_code)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    with open('å¤šæ•°æ®æºæµ‹è¯•ç»“æœ.txt', 'w', encoding='utf-8') as f:
        f.write("å¤šæ•°æ®æºæµ‹è¯•ç»“æœ\n")
        f.write("="*50 + "\n")
        f.write(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"è‚¡ç¥¨ä»£ç : {stock_code}\n\n")
        
        f.write("æ•°æ®æºç»Ÿè®¡:\n")
        for source, count in source_stats.items():
            f.write(f"  {source}: {count} æ¡\n")
        
        f.write(f"\næ€»è®¡: {sum(source_stats.values())} æ¡\n")
        f.write(f"å»é‡å: {len(unique_news)} æ¡\n\n")
        
        f.write("è¯¦ç»†æ–°é—»åˆ—è¡¨:\n")
        for i, (date, title, source) in enumerate(unique_news):
            f.write(f"{i+1}. [{source}] {date}: {title}\n")
    
    print(f"\nğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: å¤šæ•°æ®æºæµ‹è¯•ç»“æœ.txt")
    print("\nâœ… å¤šæ•°æ®æºæµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    main() 